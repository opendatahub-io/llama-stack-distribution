# WARNING: This file is auto-generated. Do not modify it manually.
# Generated by: distribution/build.py
FROM registry.access.redhat.com/ubi9/python-312@sha256:95ec8d3ee9f875da011639213fd254256c29bc58861ac0b11f290a291fa04435
WORKDIR /opt/app-root
RUN pip install uv
RUN pip install sqlalchemy # somehow sqlalchemy[asyncio] is not sufficient
RUN uv pip install --prerelease=allow --upgrade \
    'kfp-kubernetes==2.14.6' \
    'pyarrow>=21.0.0' \
    'botocore==1.35.88' \
    'boto3==1.35.88' \
    'aiobotocore==2.16.1' \
    'ibm-cos-sdk-core==2.14.2' \
    'ibm-cos-sdk==2.14.2'
RUN uv pip install --prerelease=allow 'llama_stack_provider_lmeval==0.4.2'
RUN uv pip install --prerelease=allow 'llama_stack_provider_ragas[inline]==0.5.1'
RUN uv pip install --prerelease=allow 'llama_stack_provider_ragas[remote]==0.5.1'
RUN uv pip install --prerelease=allow 'llama_stack_provider_trustyai_fms==0.3.2'
RUN uv pip install --prerelease=allow 'llama_stack_provider_trustyai_garak==0.1.8'
RUN uv pip install --prerelease=allow nltk sqlalchemy[asyncio] google-cloud-aiplatform tqdm numpy 'fonttools>=4.60.2' chardet 'datasets>=4.0.0' 'mcp>=1.23.0' asyncpg safetensors redis aiosqlite matplotlib litellm scipy requests qdrant-client psycopg2-binary sentencepiece transformers pillow pandas boto3 faiss-cpu tokenizers scikit-learn pymongo pypdf autoevals 'pymilvus[milvus-lite]>=2.4.10' 'pymilvus[milvus-lite]>=2.4.10' einops aiosqlite fastapi fire httpx uvicorn opentelemetry-sdk opentelemetry-exporter-otlp-proto-http
RUN uv pip install --prerelease=allow torch torchvision 'torchao>=0.12.0' --extra-index-url https://download.pytorch.org/whl/cpu
RUN uv pip install --prerelease=allow sentence-transformers --no-deps
RUN uv pip install --no-cache --no-deps git+https://github.com/opendatahub-io/llama-stack.git@v0.4.2+rhai0
RUN uv pip install --no-cache --no-deps llama-stack-client==v0.4.2
RUN mkdir -p ${HOME}/.llama ${HOME}/.cache
COPY distribution/config.yaml ${APP_ROOT}/config.yaml
COPY --chmod=755 distribution/entrypoint.sh ${APP_ROOT}/entrypoint.sh
#TODO: remove this once we have a stable version of llama-stack
# LLS server version is not aligned with the client version, so we disable the version check
# Currently, LLS client version is 0.3.0, while the server version is 0.3.0rc3+rhai0
ENV LLAMA_STACK_DISABLE_VERSION_CHECK=true
ENTRYPOINT [ "/opt/app-root/entrypoint.sh" ]
