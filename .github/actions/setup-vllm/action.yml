name: Setup VLLM
description: Start VLLM
runs:
  using: "composite"
  steps:
    - name: Start VLLM
      shell: bash
      run: |
        # Set VLLM_ARGS based on VLLM_MODE
        if [[ "$VLLM_MODE" == "inference" ]]; then
          VLLM_ARGS="--host 0.0.0.0 --port 8000 --enable-auto-tool-choice --tool-call-parser hermes --model /root/.cache/Qwen/Qwen3-0.6B --served-model-name Qwen/Qwen3-0.6B --max-model-len 8192"
          VLLM_PORT=8000
        elif [[ "$VLLM_MODE" == "embedding" ]]; then
          VLLM_ARGS="--host 0.0.0.0 --port 8001 --model /root/.cache/ibm-granite/granite-embedding-125m-english --served-model-name ibm-granite/granite-embedding-125m-english"
          VLLM_PORT=8001
        elif [[ "$VLLM_MODE" == "legacy" ]]; then
          VLLM_ARGS="--host 0.0.0.0 --port 8000 --enable-auto-tool-choice --tool-call-parser hermes --model /root/.cache/Qwen3-0.6B --served-model-name Qwen/Qwen3-0.6B --max-model-len 8192"
          VLLM_PORT=8000
        else
          echo "Error: VLLM_MODE must be set to 'inference' or 'embedding' or 'legacy'"
          exit 1
        fi

        # Start vllm container
        docker run -d \
          --name vllm-$VLLM_MODE \
          --privileged=true \
          --net=host \
          $VLLM_IMAGE \
          $VLLM_ARGS

        echo "Waiting for vllm to be ready on port $VLLM_PORT..."
        timeout 900 bash -c "until curl -fsS http://localhost:$VLLM_PORT/health >/dev/null; do
          echo 'Waiting for vllm...'
          sleep 5
        done"
