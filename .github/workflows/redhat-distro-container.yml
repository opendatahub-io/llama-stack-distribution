name: Build, test, and publish Red Hat Distribution Containers

on:
  pull_request:
    branches:
      - main
      - rhoai-v*
      - konflux-poc*
    types:
      - opened
      - synchronize
    paths:
      - '.github/actions/setup-vllm/action.yml'
      - '.github/workflows/redhat-distro-container.yml'
      - 'distribution/**'
      - 'tests/**'
  push:
    branches:
      - main
      - rhoai-v*

  # build a custom image from an arbitrary llama-stack commit
  workflow_dispatch:
    inputs:
      llama_stack_commit_sha:
        description: 'Llama Stack commit SHA to build from - accept long and short commit SHAs'
        required: true
        type: string

  # do a nightly test of the `main` branch of llama-stack at 6AM UTC every morning
  schedule:
    - cron: '0 6 * * *'

concurrency:
  group: ${{ github.workflow }}-${{ github.event_name }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

env:
  REGISTRY: quay.io
  IMAGE_NAME: quay.io/opendatahub/llama-stack # tags for the image will be added dynamically

jobs:
  # Test job runs integration tests for both vLLM and Vertex AI providers using matrix strategy
  # Uses fail-fast: false so both providers run independently - failure in one doesn't cancel the other
  test:
    runs-on: ubuntu-latest
    env:
      INFERENCE_MODEL: Qwen/Qwen3-0.6B
      EMBEDDING_MODEL: ibm-granite/granite-embedding-125m-english
      VLLM_URL: http://localhost:8000/v1
      LLAMA_STACK_COMMIT_SHA: ${{ github.event.inputs.llama_stack_commit_sha || 'main' }}
      VERTEX_AI_PROJECT: ${{ secrets.VERTEX_AI_PROJECT }}
      VERTEX_AI_LOCATION: ${{ secrets.VERTEX_AI_LOCATION != '' && secrets.VERTEX_AI_LOCATION || 'us-central1' }}
      GCP_WORKLOAD_IDENTITY_PROVIDER: ${{ secrets.GCP_WORKLOAD_IDENTITY_PROVIDER }}
      IMAGE_TAG: ${{ (github.event_name == 'workflow_dispatch' || github.event_name == 'schedule') && format('source-{0}-{1}', github.event.inputs.llama_stack_commit_sha || 'main', github.sha) || github.sha }}
    strategy:
      # fail-fast: false ensures both provider tests run independently
      # If vLLM tests fail, Vertex AI tests still run (and vice versa)
      # This provides better visibility into which provider has issues
      fail-fast: false
      matrix:
        # Matrix strategy runs tests for both providers in parallel
        # Each matrix entry gets its own job with provider-specific steps
        include:
          - provider: vllm
            platform: linux/amd64
          - provider: vertex
            platform: linux/amd64

    steps:
      - name: Checkout repository
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Install uv
        uses: astral-sh/setup-uv@1e862dfacbd1d6d858c55d9b792c756523627244 # v7.1.4
        with:
          python-version: 3.12
          version: 0.7.6

      - name: Set up QEMU
        uses: docker/setup-qemu-action@c7c53464625b32c7a7e944ae62b3e17d2b600130 # v3.7.0

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@e468171a9de216ec08956ac3ada2f0791b6bd435 # v3.11.1

      - name: Generate Containerfile to build an image from an arbitrary llama-stack commit (workflow_dispatch/schedule)
        if: contains(fromJSON('["workflow_dispatch", "schedule"]'), github.event_name)
        env:
          LLAMA_STACK_VERSION: ${{ env.LLAMA_STACK_COMMIT_SHA }}
        run: |
          tmp_build_dir=$(mktemp -d)
          git clone --filter=blob:none --no-checkout https://github.com/opendatahub-io/llama-stack.git "$tmp_build_dir"
          cd "$tmp_build_dir"
          git checkout "$LLAMA_STACK_VERSION"
          python3 -m venv .venv
          source .venv/bin/activate
          pip install --no-cache -e .
          cd -
          python3 distribution/build.py
          sed -i '/^RUN pip install --no-cache llama-stack==/d' distribution/Containerfile

      - name: Build image
        id: build
        uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83 # v6.18.0
        with:
          context: .
          file: distribution/Containerfile
          platforms: ${{ matrix.platform }}
          push: false
          tags: ${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }}
          load: true  # needed to load for smoke/integration tests
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Start VLLM
        if: github.event_name != 'workflow_dispatch' && matrix.provider == 'vllm'
        uses: ./.github/actions/setup-vllm

      - name: Start and smoke test LLS distro image (vLLM)
        if: github.event_name != 'workflow_dispatch' && matrix.provider == 'vllm'
        id: smoke-test
        shell: bash
        run: ./tests/smoke.sh

      - name: Integration tests (vLLM)
        if: github.event_name != 'workflow_dispatch' && matrix.provider == 'vllm'
        id: integration-tests-vllm
        shell: bash
        run: ./tests/run_integration_tests.sh

      # Vertex AI provider steps - only run when secrets are configured
      # Note: Subsequent steps have conditional checks, so they'll automatically skip if secrets are missing
      - name: Validate Vertex AI configuration
        if: github.event_name != 'workflow_dispatch' && matrix.provider == 'vertex'
        shell: bash
        run: |
          if [ -z "${{ env.VERTEX_AI_PROJECT }}" ] || [ -z "${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER }}" ]; then
            echo "âš ï¸ Vertex AI secrets not configured. Vertex AI test steps will be skipped."
            echo "To enable Vertex AI tests, configure these repository secrets:"
            echo "  - VERTEX_AI_PROJECT: Your GCP project ID"
            echo "  - GCP_WORKLOAD_IDENTITY_PROVIDER: OIDC workload identity provider"
          else
            echo "âœ… Vertex AI secrets configured. Proceeding with Vertex AI tests."
          fi

      - name: Authenticate to Google Cloud
        if: github.event_name != 'workflow_dispatch' && matrix.provider == 'vertex' && env.VERTEX_AI_PROJECT != '' && env.GCP_WORKLOAD_IDENTITY_PROVIDER != ''
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER }}

      - name: Set up Cloud SDK
        if: github.event_name != 'workflow_dispatch' && matrix.provider == 'vertex' && env.VERTEX_AI_PROJECT != '' && env.GCP_WORKLOAD_IDENTITY_PROVIDER != ''
        uses: google-github-actions/setup-gcloud@v2

      - name: Configure gcloud
        if: github.event_name != 'workflow_dispatch' && matrix.provider == 'vertex' && env.VERTEX_AI_PROJECT != '' && env.GCP_WORKLOAD_IDENTITY_PROVIDER != ''
        run: |
          gcloud config set project "${{ env.VERTEX_AI_PROJECT }}"
          gcloud config set compute/region "${{ env.VERTEX_AI_LOCATION }}"

      - name: Start Llama Stack container (Vertex)
        if: github.event_name != 'workflow_dispatch' && matrix.provider == 'vertex' && env.VERTEX_AI_PROJECT != '' && env.GCP_WORKLOAD_IDENTITY_PROVIDER != ''
        shell: bash
        run: |
          echo "Starting Llama Stack container with Vertex AI provider..."
          docker run -d --net=host -p 8321:8321 \
            -v "$HOME/.config/gcloud:/root/.config/gcloud:ro" \
            -e VERTEX_AI_PROJECT="${{ env.VERTEX_AI_PROJECT }}" \
            -e VERTEX_AI_LOCATION="${{ env.VERTEX_AI_LOCATION }}" \
            -e GOOGLE_APPLICATION_CREDENTIALS="" \
            --name llama-stack-vertex \
            "${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }}"
          
          # Health check: wait up to 60 seconds for server to be ready
          # This ensures the container is fully started before running tests
          echo "Waiting for Vertex-backed Llama Stack to be ready..."
          for i in {1..60}; do
            curl -fsS http://127.0.0.1:8321/v1/health 2>/dev/null | grep -q '"status":"OK"' && break
            if [ "$i" -eq 60 ]; then
              docker logs llama-stack-vertex || true
              docker rm -f llama-stack-vertex || true
              exit 1
            fi
            sleep 1
          done

      - name: Integration tests (Vertex)
        if: github.event_name != 'workflow_dispatch' && matrix.provider == 'vertex' && env.VERTEX_AI_PROJECT != '' && env.GCP_WORKLOAD_IDENTITY_PROVIDER != ''
        id: integration-tests-vertex
        shell: bash
        run: ./tests/run_integration_tests.sh

      - name: Gather logs and debugging information
        if: always() && github.event_name != 'workflow_dispatch'
        shell: bash
        run: |
          TARGET_DIR="logs/${{ matrix.provider }}"
          mkdir -p "$TARGET_DIR"

          docker logs llama-stack > "$TARGET_DIR/llama-stack.log" 2>&1 || true
          docker logs vllm > "$TARGET_DIR/vllm.log" 2>&1 || true
          docker logs llama-stack-vertex > "$TARGET_DIR/llama-stack-vertex.log" 2>&1 || true

          {
            echo "Disk usage:" && df -h
            echo "Memory usage:" && free -h
            echo "Docker images:" && docker images
            echo "Docker containers:" && docker ps -a
          } > "$TARGET_DIR/system-info.log" 2>&1

          if [ -d "/tmp/llama-stack-integration-tests" ]; then
            find /tmp/llama-stack-integration-tests -name "*.log" -o -name "pytest.log" -o -name "*.out" 2>/dev/null | while read -r file; do
              cp "$file" "$TARGET_DIR/$(basename "$file")" || true
            done
          fi

      - name: Upload logs as artifacts
        if: always() && github.event_name != 'workflow_dispatch'
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5.0.0
        with:
          name: ci-logs-${{ matrix.provider }}-${{ github.sha }}
          path: logs/${{ matrix.provider }}/
          retention-days: 7

      - name: cleanup
        if: always()
        shell: bash
        run: |
          docker rm -f vllm llama-stack llama-stack-vertex >/dev/null 2>&1 || true

  # Publish job runs only after all test matrix jobs succeed
  # Separated from test job to ensure images are only published when all providers pass
  # This prevents publishing broken images if one provider fails while another succeeds
  publish:
    needs: test
    if: contains(fromJSON('["push", "workflow_dispatch"]'), github.event_name)
    runs-on: ubuntu-latest
    env:
      LLAMA_STACK_COMMIT_SHA: ${{ github.event.inputs.llama_stack_commit_sha || 'main' }}
      IMAGE_TAG: ${{ (github.event_name == 'workflow_dispatch' || github.event_name == 'schedule') && format('source-{0}-{1}', github.event.inputs.llama_stack_commit_sha || 'main', github.sha) || github.sha }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Install uv
        uses: astral-sh/setup-uv@1e862dfacbd1d6d858c55d9b792c756523627244 # v7.1.4
        with:
          python-version: 3.12
          version: 0.7.6

      - name: Set up QEMU
        uses: docker/setup-qemu-action@c7c53464625b32c7a7e944ae62b3e17d2b600130 # v3.7.0

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@e468171a9de216ec08956ac3ada2f0791b6bd435 # v3.11.1

      - name: Generate Containerfile to build an image from an arbitrary llama-stack commit (workflow_dispatch/schedule)
        if: contains(fromJSON('["workflow_dispatch", "schedule"]'), github.event_name)
        env:
          LLAMA_STACK_VERSION: ${{ env.LLAMA_STACK_COMMIT_SHA }}
        run: |
          tmp_build_dir=$(mktemp -d)
          git clone --filter=blob:none --no-checkout https://github.com/llamastack/llama-stack.git "$tmp_build_dir"
          cd "$tmp_build_dir"
          git checkout "$LLAMA_STACK_VERSION"
          python3 -m venv .venv
          source .venv/bin/activate
          pip install --no-cache -e .
          cd -
          python3 distribution/build.py
          sed -i '/^RUN pip install --no-cache llama-stack==/d' distribution/Containerfile

      - name: Log in to Quay.io
        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef # v3.6.0
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ secrets.QUAY_USERNAME }}
          password: ${{ secrets.QUAY_PASSWORD }}

      - name: Publish image to Quay.io
        uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83 # v6.18.0
        with:
          context: .
          file: distribution/Containerfile
          platforms: linux/amd64
          push: true
          tags: ${{ github.event_name == 'workflow_dispatch' && format('{0}:source-{1}-{2}', env.IMAGE_NAME, env.LLAMA_STACK_COMMIT_SHA, github.sha) || format('{0}:{1}{2}', env.IMAGE_NAME, github.sha, github.ref == 'refs/heads/main' && format(',{0}:latest', env.IMAGE_NAME) || (startsWith(github.ref, 'refs/heads/rhoai-v') && format(',{0}:{1}-latest', env.IMAGE_NAME, github.ref_name)) || '') }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Output custom build information
        if: contains(fromJSON('["workflow_dispatch", "schedule"]'), github.event_name)
        run: |
          echo "âœ… Custom container image built successfully!"
          echo "ðŸ“¦ Image: ${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }}"
          echo "ðŸ”— Llama Stack commit: ${{ env.LLAMA_STACK_COMMIT_SHA }}"
          echo ""
          echo "You can pull this image using:"
          echo "docker pull ${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }}"
