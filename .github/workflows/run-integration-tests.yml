name: Run integration tests

on:
  pull_request:
    branches:
      - main
      - rhoai-v*
      - konflux-poc*
    types:
      - opened
      - synchronize
    paths:
      - '.github/actions/setup-vllm/action.yml'
      - '.github/workflows/run-integration-tests.yml'
      - 'distribution/**'
      - 'tests/**'
  push:
    branches:
      - main
      - rhoai-v*
  workflow_dispatch:
    inputs:
      llama_stack_commit_sha:
        description: 'Llama Stack commit SHA to test against - accept long and short commit SHAs'
        required: false
        type: string
        default: 'main'
  schedule:
    - cron: '0 6 * * *'

concurrency:
  group: ${{ github.workflow }}-${{ github.event_name }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

env:
  REGISTRY: quay.io
  IMAGE_NAME: quay.io/opendatahub/llama-stack
  IMAGE_TAG: ${{ github.sha }}

jobs:
  # Single job with sequential test execution
  # Deploys Llama Stack once with both vLLM and Vertex AI support
  # Then runs vLLM tests, followed by Vertex AI tests, using the same deployed instance
  test:
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    env:
      LLAMA_STACK_COMMIT_SHA: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.llama_stack_commit_sha || 'main' }}
      VERTEX_AI_PROJECT: ${{ secrets.VERTEX_AI_PROJECT }}
      VERTEX_AI_LOCATION: us-central1
      GCP_WORKLOAD_IDENTITY_PROVIDER: ${{ secrets.GCP_WORKLOAD_IDENTITY_PROVIDER }}
      # Deployment configuration - Llama Stack will support both vLLM and Vertex AI
      DEPLOY_INFERENCE_MODEL: Qwen/Qwen3-0.6B
      DEPLOY_EMBEDDING_MODEL: ibm-granite/granite-embedding-125m-english
      DEPLOY_VLLM_URL: http://localhost:8000/v1
      # Test configurations
      VLLM_INFERENCE_MODEL: Qwen/Qwen3-0.6B
      VLLM_EMBEDDING_MODEL: ibm-granite/granite-embedding-125m-english
      VERTEXAI_INFERENCE_MODEL: google/gemini-2.0-flash
      VERTEXAI_EMBEDDING_MODEL: ibm-granite/granite-embedding-125m-english

    steps:
      - name: Checkout repository
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Install uv
        uses: astral-sh/setup-uv@1e862dfacbd1d6d858c55d9b792c756523627244 # v7.1.4
        with:
          python-version: 3.12
          version: 0.7.6

      - name: Set up QEMU
        uses: docker/setup-qemu-action@c7c53464625b32c7a7e944ae62b3e17d2b600130 # v3.7.0

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@e468171a9de216ec08956ac3ada2f0791b6bd435 # v3.11.1

      # ============================================
      # Start vLLM early (non-blocking)
      # ============================================
      # Start vLLM container early so it can start up in parallel with image build
      # We'll validate it's ready before running vLLM tests

      - name: Start VLLM (early, non-blocking)
        if: github.event_name != 'workflow_dispatch'
        uses: ./.github/actions/setup-vllm

      # ============================================
      # Deployment Section
      # ============================================
      # Deploy Llama Stack once with both vLLM and Vertex AI support
      # This single instance will be used for all tests

      - name: Generate Containerfile to build an image from an arbitrary llama-stack commit (workflow_dispatch/schedule)
        if: contains(fromJSON('["workflow_dispatch", "schedule"]'), github.event_name)
        env:
          LLAMA_STACK_VERSION: ${{ env.LLAMA_STACK_COMMIT_SHA }}
        run: |
          tmp_build_dir=$(mktemp -d)
          git clone --filter=blob:none --no-checkout https://github.com/opendatahub-io/llama-stack.git "$tmp_build_dir"
          cd "$tmp_build_dir"
          git checkout "$LLAMA_STACK_VERSION"
          uv venv .venv
          source .venv/bin/activate
          uv pip install --no-cache -e .
          cd -
          python3 distribution/build.py
          sed -i '/^RUN pip install --no-cache llama-stack==/d' distribution/Containerfile

      - name: Build image
        if: github.event_name != 'workflow_dispatch'
        id: build
        uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83 # v6.18.0
        with:
          context: .
          file: distribution/Containerfile
          platforms: linux/amd64
          push: false
          tags: ${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }}
          load: true  # needed to load for smoke/integration tests
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Authenticate to Google Cloud (Vertex)
        if: github.event_name != 'workflow_dispatch' && env.VERTEX_AI_PROJECT != '' && env.GCP_WORKLOAD_IDENTITY_PROVIDER != ''
        uses: google-github-actions/auth@v3
        with:
          project_id: ${{ env.VERTEX_AI_PROJECT }}
          workload_identity_provider: ${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER }}

      - name: Set up Cloud SDK (Vertex)
        if: github.event_name != 'workflow_dispatch' && env.VERTEX_AI_PROJECT != '' && env.GCP_WORKLOAD_IDENTITY_PROVIDER != ''
        uses: google-github-actions/setup-gcloud@v2

      - name: Start Llama Stack server
        if: github.event_name != 'workflow_dispatch'
        uses: ./.github/actions/setup-llama-stack
        with:
          image_name: ${{ env.IMAGE_NAME }}
          image_tag: ${{ env.IMAGE_TAG }}
          inference_model: ${{ env.DEPLOY_INFERENCE_MODEL }}
          embedding_model: ${{ env.DEPLOY_EMBEDDING_MODEL }}
          vllm_url: ${{ env.DEPLOY_VLLM_URL }}
          vertex_ai_project: ${{ env.VERTEX_AI_PROJECT }}
          vertex_ai_location: ${{ env.VERTEX_AI_LOCATION }}

      - name: Verify deployment
        if: github.event_name != 'workflow_dispatch'
        shell: bash
        run: |
          echo "Verifying deployed Llama Stack instance..."
          curl -fsS http://127.0.0.1:8321/v1/health || exit 1
          echo "Deployment verified successfully!"

      # ============================================
      # vLLM Testing Section
      # ============================================

      - name: Validate vLLM is ready
        if: github.event_name != 'workflow_dispatch'
        shell: bash
        run: |
          echo "Validating vLLM is ready..."
          for i in {1..60}; do
            if curl -fsS http://localhost:8000/health >/dev/null 2>&1; then
              echo "vLLM is ready!"
              exit 0
            fi
            echo "Waiting for vLLM... ($i/60)"
            sleep 2
          done
          echo "vLLM failed to start after 120 seconds"
          docker logs vllm || true
          exit 1

      - name: Start and smoke test LLS distro image (vLLM)
        if: github.event_name != 'workflow_dispatch'
        id: smoke-test-vllm
        shell: bash
        env:
          INFERENCE_MODEL: ${{ env.VLLM_INFERENCE_MODEL }}
          EMBEDDING_MODEL: ${{ env.VLLM_EMBEDDING_MODEL }}
        run: ./tests/smoke.sh

      - name: Integration tests (vLLM)
        if: github.event_name != 'workflow_dispatch'
        id: integration-tests-vllm
        shell: bash
        env:
          INFERENCE_MODEL: vllm-inference/${{ env.VLLM_INFERENCE_MODEL }}
          EMBEDDING_MODEL: ${{ env.VLLM_EMBEDDING_MODEL }}
        run: |
          echo "Running integration tests for vLLM with model ${{ env.VLLM_INFERENCE_MODEL }}"
          ./tests/run_integration_tests.sh

      - name: Stop vLLM after tests
        if: always() && github.event_name != 'workflow_dispatch'
        shell: bash
        run: |
          echo "Stopping vLLM container..."
          docker rm -f vllm >/dev/null 2>&1 || true
          echo "vLLM stopped"

      # ============================================
      # Vertex AI Testing Section
      # ============================================

      - name: Verify GCP credentials for Vertex AI
        if: github.event_name != 'workflow_dispatch' && env.VERTEX_AI_PROJECT != '' && env.GCP_WORKLOAD_IDENTITY_PROVIDER != ''
        shell: bash
        run: |
          echo "Verifying GCP credentials are accessible..."
          if [ -f "$HOME/.config/gcloud/application_default_credentials.json" ]; then
            echo "✓ GCP credentials file exists on runner"
            echo "Checking if credentials are accessible in container at /run/secrets/gcp-credentials..."
            if docker exec llama-stack test -f /run/secrets/gcp-credentials 2>/dev/null; then
              echo "✓ GCP credentials are accessible in container"
              echo "Verifying GOOGLE_APPLICATION_CREDENTIALS environment variable..."
              docker exec llama-stack sh -c 'echo "GOOGLE_APPLICATION_CREDENTIALS=$GOOGLE_APPLICATION_CREDENTIALS"' || true
            else
              echo "⚠️ Warning: GCP credentials not found in container at /run/secrets/gcp-credentials"
              echo "Container logs:"
              docker logs llama-stack | tail -20 || true
            fi
          else
            echo "⚠️ Warning: GCP credentials file not found on runner"
            echo "Vertex AI tests may fail"
          fi

      - name: Integration tests (Vertex AI)
        if: github.event_name != 'workflow_dispatch' && env.VERTEX_AI_PROJECT != '' && env.GCP_WORKLOAD_IDENTITY_PROVIDER != ''
        id: integration-tests-vertexai
        shell: bash
        env:
          INFERENCE_MODEL: vertexai/${{ env.VERTEXAI_INFERENCE_MODEL }}
          EMBEDDING_MODEL: ${{ env.VERTEXAI_EMBEDDING_MODEL }}
        run: |
          echo "Running integration tests for Vertex AI with model ${{ env.VERTEXAI_INFERENCE_MODEL }}"
          ./tests/run_integration_tests.sh

      # ============================================
      # Log Collection and Cleanup
      # ============================================

      - name: Gather logs and debugging information
        if: always() && github.event_name != 'workflow_dispatch'
        shell: bash
        run: |
          mkdir -p logs

          docker logs llama-stack > logs/llama-stack.log 2>&1 || true
          docker logs vllm > logs/vllm.log 2>&1 || true

          {
            echo "Disk usage:" && df -h
            echo "Memory usage:" && free -h
            echo "Docker images:" && docker images
            echo "Docker containers:" && docker ps -a
          } > logs/system-info.log 2>&1

          if [ -d "/tmp/llama-stack-integration-tests" ]; then
            find /tmp/llama-stack-integration-tests -name "*.log" -o -name "pytest.log" -o -name "*.out" 2>/dev/null | while read -r file; do
              cp "$file" "logs/$(basename "$file")" || true
            done
          fi

      - name: Upload logs as artifacts
        if: always() && github.event_name != 'workflow_dispatch'
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5.0.0
        with:
          name: ci-logs-${{ github.sha }}
          path: logs/
          retention-days: 7

      - name: Cleanup containers
        if: always() && github.event_name != 'workflow_dispatch'
        shell: bash
        run: |
          # vLLM is already stopped after vLLM tests, only clean up llama-stack here
          docker rm -f llama-stack >/dev/null 2>&1 || true
          # Also clean up vLLM if it's still running (safety check)
          docker rm -f vllm >/dev/null 2>&1 || true
