FROM public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo AS base

WORKDIR /workspace/

RUN uv pip install "huggingface-hub[cli]"

ARG INFERENCE_MODEL=""
ARG EMBEDDING_MODEL=""

ENV INFERENCE_MODEL="${INFERENCE_MODEL}"
ENV EMBEDDING_MODEL="${EMBEDDING_MODEL}"
ENV MODEL_CACHE_DIR="/root/.cache"

RUN if [ -z "${INFERENCE_MODEL}" ]; then \
        echo "ERROR: INFERENCE_MODEL build argument is required" >&2 && exit 1; \
    fi && \
    if [ -z "${EMBEDDING_MODEL}" ]; then \
        echo "ERROR: EMBEDDING_MODEL build argument is required" >&2 && exit 1; \
    fi

RUN --mount=type=secret,id=hf_token \
    for model in "${INFERENCE_MODEL}" "${EMBEDDING_MODEL}"; do \
        model_path="${MODEL_CACHE_DIR}/${model}" && \
        mkdir -p "${model_path}" && \
        if [ -f /run/secrets/hf_token ]; then \
            HF_TOKEN=$(cat /run/secrets/hf_token) && \
            hf download "${model}" --local-dir "${model_path}" --token "${HF_TOKEN}"; \
        else \
            hf download "${model}" --local-dir "${model_path}"; \
        fi && \
        rm -rf /root/.cache/huggingface "${model_path}/original"; \
    done

ENTRYPOINT ["vllm", "serve"]
